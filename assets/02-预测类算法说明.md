# 预测类算法说明文档

## 概述

预测类算法是数学建模中的重要算法类型，用于根据历史数据对未来趋势进行预测。预测问题广泛存在于经济预测、人口预测、需求预测、时间序列分析等领域。

---

## 1. 灰色预测模型 (Grey Prediction Model, GM)

### 算法介绍

灰色预测模型用于处理"小样本、贫信息"的不确定系统。最常用的是GM(1,1)模型，通过对原始数据进行累加生成（AGO）来弱化随机性。

**GM(1,1)模型**：

原始序列：$X^{(0)} = \{x^{(0)}(1), x^{(0)}(2), \ldots, x^{(0)}(n)\}$

一次累加生成：$X^{(1)} = \{x^{(1)}(1), x^{(1)}(2), \ldots, x^{(1)}(n)\}$

其中 $x^{(1)}(k) = \sum_{i=1}^{k} x^{(0)}(i)$

白化微分方程：$\frac{dx^{(1)}}{dt} + ax^{(1)} = b$

**参数估计**：
$$
\begin{bmatrix} a \\ b \end{bmatrix} = (B^T B)^{-1} B^T Y
$$

其中：
$$
B = \begin{bmatrix} -\frac{1}{2}[x^{(1)}(1) + x^{(1)}(2)] & 1 \\ -\frac{1}{2}[x^{(1)}(2) + x^{(1)}(3)] & 1 \\ \vdots & \vdots \\ -\frac{1}{2}[x^{(1)}(n-1) + x^{(1)}(n)] & 1 \end{bmatrix}, \quad Y = \begin{bmatrix} x^{(0)}(2) \\ x^{(0)}(3) \\ \vdots \\ x^{(0)}(n) \end{bmatrix}
$$

**预测公式**：
$$
\hat{x}^{(0)}(k+1) = (1 - e^{a})[x^{(0)}(1) - \frac{b}{a}]e^{-ak}, \quad k = 1, 2, \ldots
$$

### 适用范围

| 题型类型 | 典型问题 | 特征 |
|---------|---------|------|
| 经济预测 | GDP预测、财政收入预测 | 短序列、增长趋势 |
| 人口预测 | 人口增长预测 | 单调变化 |
| 能源预测 | 电力负荷预测 | 非负数据 |
| 环境预测 | 污染物浓度预测 | 小样本 |
| 交通预测 | 交通流量预测 | 趋势明显 |

### 可视化图表类型

- **拟合曲线图**：实际值与预测值对比
- **残差图**：预测误差分布
- **相对误差图**：各点预测相对误差
- **置信区间图**：预测的置信区间

### 模型检验

| 检验类型 | 判定标准 | 说明 |
|---------|---------|------|
| 残差检验 | 平均相对误差 < 5% | 精度检验 |
| 关联度检验 | 关联度 > 0.6 | 趋势一致性 |
| 后验差检验 | C < 0.35, P > 0.95 | 方差比与小误差概率 |

### 关键文献

| 论文名称 | 作者 | 年份 | 来源 |
|---------|------|------|------|
| 灰色系统理论教程 | 邓聚龙 | 1990 | 华中理工大学出版社 |
| Grey Prediction Theory and Its Applications | 刘思峰 | 2010 | Springer |

### 代码实现要点

```python
import numpy as np

def grey_prediction_gm11(data, predict_steps=1):
    """
    GM(1,1)灰色预测模型
    :param data: 原始数据序列
    :param predict_steps: 预测步数
    :return: 预测值
    """
    data = np.array(data)
    n = len(data)

    # 一次累加生成
    agodata = np.cumsum(data)

    # 构造矩阵B和向量Y
    B = np.zeros((n-1, 2))
    Y = data[1:]

    for i in range(n-1):
        B[i, 0] = -(agodata[i] + agodata[i+1]) / 2
        B[i, 1] = 1

    # 参数估计
    params = np.linalg.inv(B.T @ B) @ B.T @ Y
    a, b = params[0], params[1]

    # 预测
    predictions = []
    for k in range(n + predict_steps):
        if k == 0:
            predictions.append(data[0])
        else:
            pred = (1 - np.exp(a)) * (data[0] - b/a) * np.exp(-a * k)
            predictions.append(pred)

    return np.array(predictions)
```

---

## 2. 插值与拟合 (Interpolation and Fitting)

### 算法介绍

插值与拟合是数据处理的基础方法，用于构造近似函数来逼近给定数据点。

#### 2.1 多项式插值

**拉格朗日插值**：
$$
L_n(x) = \sum_{i=0}^{n} y_i \prod_{j \neq i} \frac{x - x_j}{x_i - x_j}
$$

**牛顿插值**：
$$
N_n(x) = f[x_0] + f[x_0, x_1](x-x_0) + \cdots + f[x_0, \ldots, x_n](x-x_0)\cdots(x-x_{n-1})
$$

#### 2.3 样条插值

**三次样条函数**：
在每个区间 $[x_i, x_{i+1}]$ 上：
$$
S_i(x) = a_i + b_i(x-x_i) + c_i(x-x_i)^2 + d_i(x-x_i)^3
$$

满足条件：
- 插值条件：$S_i(x_i) = y_i$, $S_i(x_{i+1}) = y_{i+1}$
- 连续性条件：一阶、二阶导数连续
- 边界条件：自然边界或固定边界

#### 2.3 最小二乘拟合

**线性最小二乘**：
$$
\min_{a,b} \sum_{i=1}^{n} [y_i - (ax_i + b)]^2
$$

**多项式拟合**：
$$
\min_{a_0,\ldots,a_m} \sum_{i=1}^{n} [y_i - \sum_{j=0}^{m} a_j x_i^j]^2
$$

**正则化拟合（岭回归）**：
$$
\min_{\beta} \|y - X\beta\|^2 + \lambda \|\beta\|^2
$$

### 适用范围

| 题型类型 | 典型问题 | 特征 |
|---------|---------|------|
| 数据补全 | 缺失值填充 | 已知部分点 |
| 曲线拟合 | 趋势分析 | 带噪声数据 |
| 函数逼近 | 数值计算 | 近似计算 |
| 数据平滑 | 信号处理 | 去除噪声 |
| 图像处理 | 图像缩放 | 像素插值 |

### 可视化图表类型

- **插值曲线图**：插值函数与原始数据点
- **误差分布图**：拟合误差的分布
- **残差图**：残差与自变量关系
- **置信带图**：拟合的置信区间

### 关键文献

| 论文名称 | 作者 | 年份 | 来源 |
|---------|------|------|------|
| A Practical Guide to Splines | Carl de Boor | 1978 | Springer |
| Numerical Analysis | Burden & Faires | 2010 | Brooks/Cole |

### 代码实现要点

```python
import numpy as np
from scipy.interpolate import interp1d, CubicSpline
from scipy.optimize import curve_fit

# 拉格朗日插值
def lagrange_interpolation(x, y, x_new):
    """拉格朗日插值"""
    n = len(x)
    y_new = np.zeros_like(x_new)
    for i in range(n):
        li = np.ones_like(x_new)
        for j in range(n):
            if i != j:
                li *= (x_new - x[j]) / (x[i] - x[j])
        y_new += y[i] * li
    return y_new

# 样条插值
def spline_interpolation(x, y, x_new):
    """三次样条插值"""
    cs = CubicSpline(x, y, bc_type='natural')
    return cs(x_new)

# 最小二乘拟合
def least_squares_fit(x, y, degree=2):
    """多项式最小二乘拟合"""
    coeffs = np.polyfit(x, y, degree)
    return np.poly1d(coeffs)
```

---

## 3. 线性回归 (Linear Regression)

### 算法介绍

线性回归用于建立自变量与因变量之间的线性关系模型。

#### 3.1 一元线性回归

**模型**：
$$
y = \beta_0 + \beta_1 x + \varepsilon
$$

**参数估计**（最小二乘）：
$$
\hat{\beta}_1 = \frac{\sum_{i=1}^{n}(x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^{n}(x_i - \bar{x})^2}
$$
$$
\hat{\beta}_0 = \bar{y} - \hat{\beta}_1 \bar{x}
$$

#### 3.2 多元线性回归

**模型**：
$$
y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_p x_p + \varepsilon
$$

**矩阵形式**：
$$
Y = X\beta + \varepsilon
$$

**参数估计**：
$$
\hat{\beta} = (X^T X)^{-1} X^T Y
$$

#### 3.3 逐步回归

**前进法**：逐步引入重要变量
**后退法**：逐步剔除不重要变量
**逐步法**：结合前进和后退

**变量选择准则**：
- AIC准则：$AIC = n \ln(RSS/n) + 2p$
- BIC准则：$BIC = n \ln(RSS/n) + p \ln n$
- 调整$R^2$：$R^2_{adj} = 1 - \frac{(1-R^2)(n-1)}{n-p-1}$

### 适用范围

| 题型类型 | 典型问题 | 特征 |
|---------|---------|------|
| 经济预测 | GDP与投资关系 | 线性关系 |
| 市场分析 | 销量与广告投入 | 多因素影响 |
| 房价预测 | 房价与特征关系 | 多变量 |
| 医学研究 | 病情与指标关系 | 因果分析 |
| 生产预测 | 产量与要素投入 | 生产函数 |

### 可视化图表类型

- **散点图+回归线**：一元回归可视化
- **残差图**：残差与预测值关系
- **QQ图**：残差正态性检验
- **偏回归图**：各自变量的影响
- **置信区间图**：预测的置信区间

### 模型评估

| 评估指标 | 公式 | 说明 |
|---------|------|------|
| $R^2$ | $1 - \frac{RSS}{TSS}$ | 拟合优度 |
| 调整$R^2$ | $1 - \frac{(1-R^2)(n-1)}{n-p-1}$ | 考虑变量数 |
| RMSE | $\sqrt{\frac{1}{n}\sum_{i=1}^{n}(y_i - \hat{y}_i)^2}$ | 均方根误差 |
| MAE | $\frac{1}{n}\sum_{i=1}^{n}\|y_i - \hat{y}_i\|$ | 平均绝对误差 |
| MAPE | $\frac{100\%}{n}\sum_{i=1}^{n}\|\frac{y_i - \hat{y}_i}{y_i}\|$ | 平均绝对百分比误差 |

### 关键文献

| 论文名称 | 作者 | 年份 | 来源 |
|---------|------|------|------|
| Applied Linear Statistical Models | Kutner et al. | 2005 | McGraw-Hill |
| Regression Modeling Strategies | Harrell | 2015 | Springer |

### 代码实现要点

```python
import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.metrics import r2_score, mean_squared_error
import statsmodels.api as sm

# 一元线性回归
def simple_linear_regression(x, y):
    """一元线性回归"""
    x = x.reshape(-1, 1)
    model = LinearRegression()
    model.fit(x, y)
    y_pred = model.predict(x)

    return {
        'intercept': model.intercept_,
        'coefficient': model.coef_[0],
        'r2': r2_score(y, y_pred),
        'predictions': y_pred
    }

# 多元线性回归（带统计检验）
def multiple_linear_regression(X, y):
    """多元线性回归（使用statsmodels）"""
    X = sm.add_constant(X)  # 添加截距项
    model = sm.OLS(y, X).fit()

    return {
        'params': model.params,
        'pvalues': model.pvalues,
        'rsquared': model.rsquared,
        'summary': model.summary()
    }

# 逐步回归
def stepwise_regression(X, y, initial_list=[], threshold_in=0.01, threshold_out=0.05):
    """逐步回归"""
    included = list(initial_list)
    while True:
        changed = False
        # 前进选择
        excluded = list(set(X.columns) - set(included))
        new_pval = pd.Series(index=excluded)
        for new_column in excluded:
            model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included + [new_column]]))).fit()
            new_pval[new_column] = model.pvalues[new_column]
        best_pval = new_pval.min()
        if best_pval < threshold_in:
            best_feature = new_pval.idxmin()
            included.append(best_feature)
            changed = True
        # 后向剔除
        model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included]))).fit()
        pvalues = model.pvalues.iloc[1:]
        worst_pval = pvalues.max()
        if worst_pval > threshold_out:
            changed = True
            worst_feature = pvalues.idxmax()
            included.remove(worst_feature)
        if not changed:
            break
    return included
```

---

## 4. 神经网络 (Neural Networks)

### 算法介绍

神经网络是一种模仿生物神经系统的计算模型，通过多层神经元进行非线性映射。

#### 4.1 前馈神经网络（BP网络）

**前向传播**：
$$
h_j = f(\sum_{i} w_{ij} x_i + b_j)
$$
$$
y_k = g(\sum_{j} v_{jk} h_j + c_k)
$$

**反向传播（BP算法）**：
输出层误差：$\delta_k = (y_k - t_k) \cdot g'(a_k)$
隐层误差：$\delta_j = \sum_{k} \delta_k v_{jk} \cdot f'(a_j)$

**权重更新**：
$$
\Delta w_{ij} = -\eta \frac{\partial E}{\partial w_{ij}} = \eta \delta_j x_i
$$

#### 4.2 RBF神经网络

**径向基函数**（高斯核）：
$$
\phi_j(x) = \exp\left(-\frac{\|x - c_j\|^2}{2\sigma_j^2}\right)
$$

**输出**：
$$
y = \sum_{j=1}^{m} w_j \phi_j(x)
$$

#### 4.3 LSTM网络（时间序列预测）

**遗忘门**：$f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f)$

**输入门**：$i_t = \sigma(W_i \cdot [h_{t-1}, x_t] + b_i)$

**候选值**：$\tilde{C}_t = \tanh(W_C \cdot [h_{t-1}, x_t] + b_C)$

**更新状态**：$C_t = f_t \odot C_{t-1} + i_t \odot \tilde{C}_t$

**输出门**：$o_t = \sigma(W_o \cdot [h_{t-1}, x_t] + b_o)$
$$h_t = o_t \odot \tanh(C_t)$$

### 适用范围

| 题型类型 | 典型问题 | 特征 |
|---------|---------|------|
| 非线性预测 | 股票价格预测 | 复杂非线性 |
| 时间序列 | 电力负荷预测 | 时序依赖 |
| 图像预测 | 图像生成 | 高维数据 |
| 语音预测 | 语音合成 | 序列数据 |
| 多变量预测 | 多维时间序列 | 多输入多输出 |

### 可视化图表类型

- **网络结构图**：神经网络层次结构
- **损失曲线**：训练过程损失变化
- **预测对比图**：实际值与预测值
- **误差热力图**：预测误差分布
- **注意力权重图**：注意力机制的权重

### 关键文献

| 论文名称 | 作者 | 年份 | 来源 |
|---------|------|------|------|
| Learning representations by back-propagating errors | Rumelhart, Hinton & Williams | 1986 | Nature |
| Long Short-Term Memory | Hochreiter & Schmidhuber | 1997 | Neural Computation |
| Adam: A Method for Stochastic Optimization | Kingma & Ba | 2014 | ICLR |

### 代码实现要点

```python
import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

# BP神经网络
def create_bpnn(input_dim, hidden_layers, output_dim):
    """创建BP神经网络"""
    model = keras.Sequential()

    # 输入层
    model.add(layers.Input(shape=(input_dim,)))

    # 隐藏层
    for units in hidden_layers:
        model.add(layers.Dense(units, activation='relu'))
        model.add(layers.Dropout(0.2))

    # 输出层
    model.add(layers.Dense(output_dim, activation='linear'))

    model.compile(optimizer='adam', loss='mse', metrics=['mae'])
    return model

# LSTM网络
def create_lstm(input_shape, lstm_units, output_dim):
    """创建LSTM网络"""
    model = keras.Sequential([
        layers.Input(shape=input_shape),
        layers.LSTM(lstm_units, return_sequences=True),
        layers.Dropout(0.2),
        layers.LSTM(lstm_units // 2),
        layers.Dropout(0.2),
        layers.Dense(output_dim)
    ])

    model.compile(optimizer='adam', loss='mse')
    return model

# 训练和预测
def train_and_predict(model, X_train, y_train, X_test, epochs=100, batch_size=32):
    """训练和预测"""
    history = model.fit(X_train, y_train,
                       epochs=epochs,
                       batch_size=batch_size,
                       validation_split=0.2,
                       verbose=0)
    predictions = model.predict(X_test)
    return history, predictions
```

---

## 5. 支持向量机 (Support Vector Machine, SVM)

### 算法介绍

支持向量机是一种强大的监督学习算法，可用于分类和回归。SVR（支持向量回归）用于预测问题。

#### 5.1 线性SVR

**原始问题**：
$$
\min_{w,b,\xi,\xi^*} \frac{1}{2}\|w\|^2 + C\sum_{i=1}^{n}(\xi_i + \xi_i^*)
$$

**约束条件**：
$$
\begin{cases}
y_i - w^T x_i - b \leq \varepsilon + \xi_i \\
w^T x_i + b - y_i \leq \varepsilon + \xi_i^* \\
\xi_i, \xi_i^* \geq 0
\end{cases}
$$

#### 5.2 非线性SVR（核方法）

**对偶问题**：
$$
\max_{\alpha, \alpha^*} -\frac{1}{2}\sum_{i,j} (\alpha_i - \alpha_i^*)(\alpha_j - \alpha_j^*)K(x_i, x_j) - \varepsilon\sum_{i}(\alpha_i + \alpha_i^*) + \sum_{i} y_i(\alpha_i - \alpha_i^*)
$$

**决策函数**：
$$
f(x) = \sum_{i=1}^{n} (\alpha_i - \alpha_i^*) K(x_i, x) + b
$$

**常用核函数**：
- 线性核：$K(x, x') = x^T x'$
- 多项式核：$K(x, x') = (x^T x' + c)^d$
- RBF核：$K(x, x') = \exp(-\gamma \|x - x'\|^2)$
- Sigmoid核：$K(x, x') = \tanh(\gamma x^T x' + r)$

### 适用范围

| 题型类型 | 典型问题 | 特征 |
|---------|---------|------|
| 回归预测 | 股价预测、房价预测 | 非线性关系 |
| 时间序列 | 负荷预测、流量预测 | 小样本 |
| 函数逼近 | 复杂函数拟合 | 高维空间 |
| 异常检测 | 故障诊断 | 单类分类 |

### 可视化图表类型

- **支持向量图**：显示支持向量点
- **ε-带图**：ε不敏感带
- **决策边界图**：分类边界（分类问题）
- **核函数效果图**：不同核函数效果对比
- **参数敏感性图**：C和ε参数影响

### 关键文献

| 论文名称 | 作者 | 年份 | 来源 |
|---------|------|------|------|
| Support Vector Networks | Cortes & Vapnik | 1995 | Machine Learning |
| A Tutorial on Support Vector Regression | Smola & Schölkopf | 2004 | Statistics and Computing |

### 代码实现要点

```python
from sklearn.svm import SVR
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import GridSearchCV
import numpy as np

# SVR回归
def svr_predict(X_train, y_train, X_test, kernel='rbf', C=1.0, epsilon=0.1):
    """支持向量回归预测"""
    # 数据标准化
    scaler_X = StandardScaler()
    scaler_y = StandardScaler()
    X_train_scaled = scaler_X.fit_transform(X_train)
    y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1)).ravel()

    # 创建SVR模型
    model = SVR(kernel=kernel, C=C, epsilon=epsilon, gamma='scale')
    model.fit(X_train_scaled, y_train_scaled)

    # 预测
    X_test_scaled = scaler_X.transform(X_test)
    y_pred_scaled = model.predict(X_test_scaled)
    y_pred = scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).ravel()

    return y_pred, model

# 参数调优
def svr_grid_search(X_train, y_train):
    """SVR参数网格搜索"""
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)

    param_grid = {
        'kernel': ['rbf', 'linear', 'poly'],
        'C': [0.1, 1, 10, 100],
        'epsilon': [0.01, 0.1, 0.2],
        'gamma': ['scale', 'auto', 0.001, 0.01, 0.1]
    }

    svr = SVR()
    grid_search = GridSearchCV(svr, param_grid, cv=5, scoring='neg_mean_squared_error')
    grid_search.fit(X_train_scaled, y_train)

    return grid_search.best_params_, grid_search.best_score_
```

---

## 6. ARIMA模型

### 算法介绍

ARIMA（自回归差分移动平均）模型是经典的时间序列预测模型。

**ARIMA(p, d, q)模型**：

**AR(p)部分**：$X_t = c + \sum_{i=1}^{p} \phi_i X_{t-i} + \varepsilon_t$

**I(d)部分**：差分$d$次实现平稳

**MA(q)部分**：$X_t = \mu + \sum_{i=1}^{q} \theta_i \varepsilon_{t-i} + \varepsilon_t$

**完整形式**（差分后）：
$$
(1 - \sum_{i=1}^{p} \phi_i L^i)(1 - L)^d X_t = (1 + \sum_{i=1}^{q} \theta_i L^i) \varepsilon_t
$$

其中$L$是滞后算子，$L X_t = X_{t-1}$。

### 适用范围

| 题型类型 | 典型问题 | 特征 |
|---------|---------|------|
| 时间序列 | 经济指标预测 | 单变量时序 |
| 销售预测 | 产品销量预测 | 趋势+季节性 |
| 金融预测 | 股价、利率预测 | 波动性 |
| 气象预测 | 气温、降雨量预测 | 周期性 |

### 可视化图表类型

- **时序图**：原始数据和预测值
- **ACF/PACF图**：自相关和偏自相关
- **残差图**：模型残差检验
- **预测区间图**：置信区间

### 关键文献

| 论文名称 | 作者 | 年份 | 来源 |
|---------|------|------|------|
| Time Series Analysis | Box & Jenkins | 1970 | Holden-Day |

### 代码实现要点

```python
from statsmodels.tsa.arima.model import ARIMA
from statsmodels.tsa.stattools import adfuller
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf

# 平稳性检验
def test_stationarity(timeseries):
    """ADF平稳性检验"""
    result = adfuller(timeseries)
    return {'ADF Statistic': result[0], 'p-value': result[1]}

# ARIMA建模
def arima_forecast(train_data, test_steps, order=(1,1,1)):
    """ARIMA预测"""
    model = ARIMA(train_data, order=order)
    fitted_model = model.fit()
    forecast = fitted_model.forecast(steps=test_steps)
    return forecast, fitted_model

# 自动选择阶数
def auto_arima(train_data, max_p=5, max_d=2, max_q=5):
    """自动ARIMA阶数选择"""
    import itertools
    import warnings

    best_aic = np.inf
    best_order = None
    warnings.filterwarnings('ignore')

    for p in range(max_p + 1):
        for d in range(max_d + 1):
            for q in range(max_q + 1):
                try:
                    model = ARIMA(train_data, order=(p, d, q))
                    results = model.fit()
                    if results.aic < best_aic:
                        best_aic = results.aic
                        best_order = (p, d, q)
                except:
                    continue

    return best_order
```

---

## 7. 指数平滑法 (Exponential Smoothing)

### 算法介绍

指数平滑法通过对历史数据加权平均进行预测，近期数据权重更大。

#### 7.1 简单指数平滑

适用于无趋势、无季节性的数据。

$$
F_{t+1} = \alpha y_t + (1 - \alpha) F_t
$$

其中$\alpha \in (0, 1)$是平滑系数。

#### 7.2 Holt线性趋势法

适用于有趋势、无季节性的数据。

**水平方程**：$L_t = \alpha y_t + (1 - \alpha)(L_{t-1} + T_{t-1})$

**趋势方程**：$T_t = \beta(L_t - L_{t-1}) + (1 - \beta)T_{t-1}$

**预测**：$F_{t+m} = L_t + m T_t$

#### 7.3 Holt-Winters季节性方法

适用于有趋势和季节性的数据。

**水平方程**：$L_t = \alpha \frac{y_t}{S_{t-s}} + (1 - \alpha)(L_{t-1} + T_{t-1})$

**趋势方程**：$T_t = \beta(L_t - L_{t-1}) + (1 - \beta)T_{t-1}$

**季节方程**：$S_t = \gamma \frac{y_t}{L_t} + (1 - \gamma)S_{t-s}$

**预测**：$F_{t+m} = (L_t + m T_t) S_{t-s+m}$

### 适用范围

| 题型类型 | 典型问题 | 特征 |
|---------|---------|------|
| 短期预测 | 销售预测 | 短期趋势 |
| 库存预测 | 需求预测 | 平稳变化 |
| 季节性预测 | 季节性商品销售 | 季节波动 |
| 平稳序列 | 指标监测 | 无明显趋势 |

### 可视化图表类型

- **时序预测图**：历史数据+预测值
- **分解图**：趋势、季节、残差分解
- **误差分析图**：预测误差分析

### 关键文献

| 论文名称 | 作者 | 年份 | 来源 |
|---------|------|------|------|
| Forecasting: principles and practice | Hyndman & Athanasopoulos | 2018 | OTexts |

### 代码实现要点

```python
from statsmodels.tsa.holtwinters import SimpleExpSmoothing, Holt, ExponentialSmoothing

# 简单指数平滑
def simple_exponential_smoothing(data, alpha=0.2):
    """简单指数平滑"""
    model = SimpleExpSmoothing(data)
    fit = model.fit(smoothing_level=alpha, optimized=False)
    forecast = fit.forecast(steps=1)
    return forecast, fit

# Holt线性趋势法
def holt_linear_trend(data, alpha=0.3, beta=0.1):
    """Holt线性趋势法"""
    model = Holt(data)
    fit = model.fit(smoothing_level=alpha, smoothing_trend=beta, optimized=False)
    forecast = fit.forecast(steps=5)
    return forecast, fit

# Holt-Winters季节性方法
def holt_winters(data, seasonal_periods=12, trend='add', seasonal='add'):
    """Holt-Winters季节性方法"""
    model = ExponentialSmoothing(data, trend=trend, seasonal=seasonal,
                                 seasonal_periods=seasonal_periods)
    fit = model.fit()
    forecast = fit.forecast(steps=seasonal_periods)
    return forecast, fit
```

---

## 8. Prophet预测模型

### 算法介绍

Prophet是Facebook开发的时间序列预测模型，适合具有季节性和节假日效应的商务时间序列。

**模型公式**：
$$
y(t) = g(t) + s(t) + h(t) + \varepsilon_t
$$

其中：
- $g(t)$：趋势项（线性或逻辑斯谛）
- $s(t)$：季节项（傅里叶级数）
- $h(t)$：节假日效应
- $\varepsilon_t$：误差项

**趋势项**（线性）：$g(t) = (k + a_k \delta(t) \cdot t + (m + a_m \delta(t))$

**季节项**：$s(t) = \sum_{n=1}^{N} [a_n \cos(\frac{2\pi n t}{P}) + b_n \sin(\frac{2\pi n t}{P})]$

### 适用范围

| 题型类型 | 典型问题 | 特征 |
|---------|---------|------|
| 商务预测 | 销售量预测 | 季节性强 |
| 流量预测 | 网站访问量 | 节假日效应 |
| 用户增长 | 用户数预测 | 趋势+周期 |
| 库存预测 | 商品需求 | 多种影响因素 |

### 可视化图表类型

- **预测分解图**：趋势、季节、节假日分解
- **预测区间图**：不确定性区间
- **组件影响图**：各组件贡献

### 关键文献

| 论文名称 | 作者 | 年份 | 来源 |
|---------|------|------|------|
| Forecasting at scale | Taylor & Letham | 2018 | arXiv |

### 代码实现要点

```python
from prophet import Prophet
import pandas as pd

def prophet_forecast(df, periods=30, yearly_seasonality=True, weekly_seasonality=True):
    """
    Prophet时间序列预测
    :param df: DataFrame with columns 'ds' (datetime) and 'y' (value)
    :param periods: 预测步数
    :return: 预测结果
    """
    # 创建模型
    model = Prophet(
        yearly_seasonality=yearly_seasonality,
        weekly_seasonality=weekly_seasonality,
        daily_seasonality=False,
        seasonality_mode='additive'
    )

    # 添加自定义季节性
    model.add_seasonality(name='monthly', period=30.5, fourier_order=5)

    # 拟合模型
    model.fit(df)

    # 创建未来日期
    future = model.make_future_dataframe(periods=periods)

    # 预测
    forecast = model.predict(future)

    return forecast, model
```

---

## 9. XGBoost/LightGBM预测模型

### 算法介绍

XGBoost和LightGBM是两种高效的梯度提升决策树算法，广泛应用于各类预测问题。

#### 9.1 XGBoost (eXtreme Gradient Boosting)

**核心思想**：通过迭代添加弱学习器（决策树）来最小化目标函数。

**目标函数**：
$$
Obj(\Theta) = \sum_{i=1}^{n} L(y_i, \hat{y}_i) + \sum_{k=1}^{K} \Omega(f_k)
$$

其中：
- $L$：损失函数（如平方损失、逻辑损失）
- $\Omega$：正则化项（控制模型复杂度）

**二阶泰勒展开**：
$$
Obj^{(t)} \approx \sum_{i=1}^{n} [L(y_i, \hat{y}_i^{(t-1)}) + g_i f_t(x_i) + \frac{1}{2} h_i f_t^2(x_i)] + \Omega(f_t)
$$

其中 $g_i = \partial_{\hat{y}^{(t-1)}} L(y_i, \hat{y}^{(t-1)})$，$h_i = \partial_{\hat{y}^{(t-1)}}^2 L(y_i, \hat{y}^{(t-1)})$

**最优目标函数值**：
$$
\tilde{Obj}^{(t)} = -\frac{1}{2} \sum_{i=1}^{n} \frac{g_i^2}{h_i + \lambda} + \gamma T
$$

#### 9.2 LightGBM (Light Gradient Boosting Machine)

**核心改进**：
- **基于直方图的决策树**：将连续特征离散化为bins，加速训练
- **带深度限制的叶子增长（Leaf-wise）**：选择增益最大的叶子分裂
- **特征捆绑**：将互斥特征捆绑降低维度
- **单边梯度采样（GOSS）**：保留大梯度样本，随机采样小梯度样本

**目标函数**：
$$
Obj(\Theta) = \sum_{i=1}^{n} g_i f_t(x_i) + \frac{1}{2} \sum_{i=1}^{n} h_i f_t^2(x_i) + \Omega(f_t)
$$

### 适用范围

| 题型类型 | 典型问题 | 特征 |
|---------|---------|------|
| 表格数据预测 | 销量预测、用户行为 | 结构化数据 |
| 时间序列预测 | 股价、负荷预测 | 序列特征 |
| 分类预测 | 违约预测、疾病诊断 | 二分类/多分类 |
| 排序预测 | 搜索排名、推荐 | 排序任务 |
| 异常检测 | 欺诈检测 | 不平衡数据 |

### 可视化图表类型

- **特征重要性图**：特征贡献排序
- **SHAP值图**：特征对预测的影响
- **学习曲线**：训练/验证误差
- **残差图**：预测误差分析
- **树可视化**：决策树结构

### 关键文献

| 论文名称 | 作者 | 年份 | 来源 |
|---------|------|------|------|
| XGBoost: A scalable tree boosting system | Chen & Guestrin | 2016 | KDD |
| LightGBM: A highly efficient gradient boosting decision tree | Ke et al. | 2017 | NeurIPS |

### 代码实现要点

```python
import xgboost as xgb
import lightgbm as lgb
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# XGBoost回归
def xgboost_regression(X_train, y_train, X_test, y_test=None,
                        params=None, n_estimators=100, early_stopping_rounds=10):
    """
    XGBoost回归预测
    :param X_train: 训练特征
    :param y_train: 训练目标
    :param X_test: 测试特征
    :param y_test: 测试目标（用于早停）
    :param params: 模型参数
    :return: 模型, 预测值
    """
    if params is None:
        params = {
            'objective': 'reg:squarederror',
            'max_depth': 6,
            'learning_rate': 0.1,
            'subsample': 0.8,
            'colsample_bytree': 0.8,
            'random_state': 42
        }

    dtrain = xgb.DMatrix(X_train, label=y_train)
    dtest = xgb.DMatrix(X_test)

    # 训练模型
    evals_result = {}
    model = xgb.train(params, dtrain, num_boost_round=n_estimators,
                      evals=[(dtrain, 'train')],
                      early_stopping_rounds=early_stopping_rounds,
                      evals_result=evals_result,
                      verbose_eval=False)

    # 预测
    y_pred = model.predict(dtest)

    return model, y_pred, evals_result

# LightGBM回归
def lightgbm_regression(X_train, y_train, X_test, y_test=None,
                         params=None, n_estimators=100, early_stopping_rounds=10):
    """
    LightGBM回归预测
    :param X_train: 训练特征
    :param y_train: 训练目标
    :param X_test: 测试特征
    :param y_test: 测试目标（用于早停）
    :param params: 模型参数
    :return: 模型, 预测值
    """
    if params is None:
        params = {
            'objective': 'regression',
            'max_depth': -1,  # 不限制深度
            'learning_rate': 0.1,
            'feature_fraction': 0.8,
            'bagging_fraction': 0.8,
            'bagging_freq': 5,
            'verbose': -1
        }

    # 创建数据集
    train_data = lgb.Dataset(X_train, label=y_train)
    valid_data = lgb.Dataset(X_test, label=y_test, reference=train_data) if y_test is not None else None

    # 训练模型
    evals_result = {}
    model = lgb.train(params, train_data, num_boost_round=n_estimators,
                      valid_sets=[train_data] if valid_data is None else [train_data, valid_data],
                      valid_names=['train'] if valid_data is None else ['train', 'valid'],
                      callbacks=[lgb.early_stopping(stopping_rounds=early_stopping_rounds, verbose=False),
                                lgb.record_evaluation(evals_result)])

    # 预测
    y_pred = model.predict(X_test, num_iteration=model.best_iteration)

    return model, y_pred, evals_result

# 绘制特征重要性
def plot_feature_importance(model, feature_names, model_type='xgboost', top_n=20):
    """绘制特征重要性"""
    if model_type == 'xgboost':
        importance = model.get_score(importance_type='weight')
        importance = pd.DataFrame({'feature': list(importance.keys()),
                                   'importance': list(importance.values())})
    else:  # lightgbm
        importance = pd.DataFrame({'feature': feature_names,
                                   'importance': model.feature_importance(importance_type='split')})

    importance = importance.sort_values('importance', ascending=False).head(top_n)

    plt.figure(figsize=(10, 6))
    plt.barh(importance['feature'][::-1], importance['importance'][::-1])
    plt.xlabel('Importance')
    plt.ylabel('Feature')
    plt.title(f'Feature Importance ({model_type})')
    plt.tight_layout()
    plt.show()

# 绘制学习曲线
def plot_learning_curve(evals_result, model_type='xgboost'):
    """绘制学习曲线"""
    plt.figure(figsize=(10, 6))

    if model_type == 'xgboost':
        train_metric = evals_result['train']['rmse']
    else:  # lightgbm
        train_metric = evals_result['train']['l2']
        if 'valid' in evals_result:
            valid_metric = evals_result['valid']['l2']
            plt.plot(valid_metric, label='Validation')

    plt.plot(train_metric, label='Training')
    plt.xlabel('Iteration')
    plt.ylabel('RMSE')
    plt.title('Learning Curve')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.show()

# 超参数调优示例
def xgboost_grid_search(X_train, y_train, X_val, y_val):
    """XGBoost网格搜索"""
    from sklearn.model_selection import GridSearchCV

    params = {
        'max_depth': [3, 5, 7],
        'learning_rate': [0.01, 0.1, 0.2],
        'n_estimators': [100, 200, 500],
        'subsample': [0.8, 0.9, 1.0]
    }

    xgb_model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)
    grid_search = GridSearchCV(xgb_model, params, cv=3, scoring='neg_mean_squared_error', n_jobs=-1)
    grid_search.fit(X_train, y_train, eval_set=[(X_val, y_val)], early_stopping_rounds=10, verbose=False)

    return grid_search.best_params_, grid_search.best_score_
```

---

## 10. 时空预测模型

### 算法介绍

时空预测模型用于同时考虑时间和空间相关性的预测问题，如交通流量预测、空气质量预测、疫情传播等。

#### 10.1 时空数据特点

**时空相关性**：
- **时间依赖**：当前状态受历史状态影响
- **空间依赖**：当前位置受邻近位置影响
- **时空交互**：时间和空间相互影响

**时空数据表示**：
$$
X \in \mathbb{R}^{N \times T \times F}
$$

其中：$N$是空间节点数，$T$是时间步长，$F$是特征维度。

#### 10.2 经典时空预测模型

**ARIMA时空扩展**：STARIMA
$$
X_t = \sum_{k=1}^{p} \Phi_k X_{t-k} + \sum_{l=1}^{q} \Theta_l \epsilon_{t-l} + \epsilon_t
$$

**时空图卷积网络**：STGCN

结合图卷积网络(GCN)和时序卷积网络(TCN)。

**图卷积层**：
$$
H^{(l+1)} = \sigma(\tilde{D}^{-\frac{1}{2}}\tilde{A}\tilde{D}^{-\frac{1}{2}} H^{(l)} W^{(l)})
$$

其中$\tilde{A} = A + I$（加入自连接），$\tilde{D}_{ii} = \sum_j \tilde{A}_{ij}$

**时间卷积层**：使用1D卷积捕获时序模式。

#### 10.3 交通流量预测示例

**基于图神经网络的交通预测**：

1. **构建道路网络图**：$G = (V, E, W)$
   - $V$：道路节点
   - $E$：道路连接
   - $W$：连接权重（距离、相似度等）

2. **时空注意力机制**：
$$
\alpha_{ij}^t = \frac{\exp(score(s_i^t, s_j^t))}{\sum_{k \in \mathcal{N}(i)} \exp(score(s_i^t, s_k^t))}
$$

3. **预测输出**：
$$
\hat{X}_{t+1:t+H} = \text{Decoder}(\text{Encoder}(X_{t-T:t}, G))
$$

#### 10.4 评价指标

**RMSE**（均方根误差）：
$$
RMSE = \sqrt{\frac{1}{NTH} \sum_{n,t,h} (X_{n,t+h} - \hat{X}_{n,t+h})^2}
$$

**MAE**（平均绝对误差）：
$$
MAE = \frac{1}{NTH} \sum_{n,t,h} |X_{n,t+h} - \hat{X}_{n,t+h}|
$$

**MAPE**（平均绝对百分比误差）：
$$
MAPE = \frac{100\%}{NTH} \sum_{n,t,h} \left|\frac{X_{n,t+h} - \hat{X}_{n,t+h}}{X_{n,t+h}}\right|
$$

### 适用范围

| 题型类型 | 典型问题 | 特征 |
|---------|---------|------|
| 交通预测 | 道路流量、地铁客流 | 路网结构 |
| 环境预测 | 空气质量、温度场 | 空间扩散 |
| 疫情预测 | 疾病传播 | 区域传播 |
| 能源预测 | 区域电力负荷 | 电网结构 |
| 经济预测 | 区域经济指标 | 空间溢出 |

### 可视化图表类型

- **时空热力图**：预测值在时空上的分布
- **路网流量图**：道路网络上的预测流量
- **等值线图**：连续空间上的预测值
- **时间序列对比**：多位置预测对比
- **误差空间分布**：预测误差的空间分布

### 关键文献

| 论文名称 | 作者 | 年份 | 来源 |
|---------|------|------|------|
| Spatio-Temporal Graph Convolutional Networks | Yu et al. | 2018 | IJCAI |
| Diffusion Convolutional Recurrent Neural Network | Li et al. | 2018 | AAAI |
| Attention Based Spatial-Temporal Graph Convolutional Networks | Guo et al. | 2019 | CIKM |

### 代码实现要点

```python
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import networkx as nx
import matplotlib.pyplot as plt

# 构建时空图
def build_spatial_graph(adj_matrix, threshold=0.1):
    """
    构建空间图
    :param adj_matrix: 邻接矩阵或距离矩阵
    :param threshold: 连接阈值
    :return: NetworkX图
    """
    G = nx.Graph()
    n = adj_matrix.shape[0]

    for i in range(n):
        for j in range(i+1, n):
            if adj_matrix[i, j] > threshold:
                G.add_edge(i, j, weight=adj_matrix[i, j])

    return G

def compute_normalized_laplacian(adj_matrix):
    """
    计算归一化拉普拉斯矩阵
    :param adj_matrix: 邻接矩阵
    :return: 归一化拉普拉斯矩阵
    """
    A = adj_matrix + np.eye(adj_matrix.shape[0])  # 自连接
    D = np.diag(A.sum(axis=1))
    D_inv_sqrt = np.linalg.inv(np.sqrt(D))
    L = np.eye(A.shape[0]) - D_inv_sqrt @ A @ D_inv_sqrt
    return torch.FloatTensor(L)

# 时空图卷积层
class STGCNBlock(nn.Module):
    """时空图卷积块"""
    def __init__(self, in_channels, out_channels, num_nodes):
        super().__init__()
        self.temporal_conv = nn.Conv2d(in_channels, out_channels, kernel_size=(1, 3), padding=(0, 1))
        self.graph_conv = GraphConv(out_channels, out_channels, num_nodes)
        self.bn = nn.BatchNorm2d(out_channels)
        self.relu = nn.ReLU()

    def forward(self, x, laplacian):
        # x: (batch, channels, time, nodes)
        x = self.temporal_conv(x)
        x = self.graph_conv(x, laplacian)
        x = self.bn(x)
        x = self.relu(x)
        return x

class GraphConv(nn.Module):
    """图卷积层"""
    def __init__(self, in_channels, out_channels, num_nodes):
        super().__init__()
        self.theta = nn.Parameter(torch.FloatTensor(in_channels, out_channels))
        nn.init.xavier_uniform_(self.theta)

    def forward(self, x, laplacian):
        # x: (batch, channels, time, nodes)
        batch, channels, time, nodes = x.shape
        x = x.permute(0, 2, 3, 1)  # (batch, time, nodes, channels)
        x = torch.matmul(x, self.theta)  # (batch, time, nodes, out_channels)
        # 图卷积: x = L @ x
        x = torch.matmul(laplacian, x)  # (batch, time, nodes, out_channels)
        x = x.permute(0, 3, 1, 2)  # (batch, out_channels, time, nodes)
        return x

# 简化的时空预测模型
class STGCNPredictor(nn.Module):
    """时空图卷积预测模型"""
    def __init__(self, input_dim, hidden_dim, output_dim, num_nodes, seq_len, pred_len):
        super().__init__()
        self.seq_len = seq_len
        self.pred_len = pred_len

        self.stgcn1 = STGCNBlock(input_dim, hidden_dim, num_nodes)
        self.stgcn2 = STGCNBlock(hidden_dim, hidden_dim, num_nodes)
        self.stgcn3 = STGCNBlock(hidden_dim, output_dim, num_nodes)

        # 输出层
        self.fc = nn.Linear(seq_len, pred_len)

    def forward(self, x, laplacian):
        # x: (batch, features, time, nodes)
        x = self.stgcn1(x, laplacian)
        x = self.stgcn2(x, laplacian)
        x = self.stgcn3(x, laplacian)

        # 时间维度映射
        batch, channels, time, nodes = x.shape
        x = x.permute(0, 3, 2, 1)  # (batch, nodes, time, channels)
        x = x.reshape(batch * nodes, time, channels)
        x = x.permute(0, 2, 1)  # (batch*nodes, channels, time)
        x = self.fc(x)  # (batch*nodes, channels, pred_len)
        x = x.permute(0, 2, 1)  # (batch*nodes, pred_len, channels)
        x = x.reshape(batch, nodes, self.pred_len, -1)
        x = x.permute(0, 3, 2, 1)  # (batch, channels, pred_len, nodes)

        return x

def train_stgcn_model(X_train, y_train, adj_matrix, epochs=100, lr=0.001):
    """
    训练时空图卷积网络
    :param X_train: (samples, features, time, nodes)
    :param y_train: (samples, features, pred_time, nodes)
    :param adj_matrix: 邻接矩阵
    :return: 训练好的模型
    """
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    # 计算拉普拉斯矩阵
    laplacian = compute_normalized_laplacian(adj_matrix).to(device)

    # 转换为张量
    X_train = torch.FloatTensor(X_train).to(device)
    y_train = torch.FloatTensor(y_train).to(device)

    # 初始化模型
    model = STGCNPredictor(
        input_dim=X_train.shape[1],
        hidden_dim=64,
        output_dim=y_train.shape[1],
        num_nodes=X_train.shape[3],
        seq_len=X_train.shape[2],
        pred_len=y_train.shape[2]
    ).to(device)

    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    criterion = nn.MSELoss()

    losses = []
    for epoch in range(epochs):
        model.train()
        optimizer.zero_grad()

        outputs = model(X_train, laplacian)
        loss = criterion(outputs, y_train)

        loss.backward()
        optimizer.step()

        losses.append(loss.item())

        if (epoch + 1) % 10 == 0:
            print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')

    return model, losses

# 绘制时空预测结果
def plot_spatiotemporal_prediction(true_values, pred_values, node_ids, times, title="时空预测结果"):
    """
    绘制时空预测结果
    :param true_values: 真实值 (time, nodes)
    :param pred_values: 预测值 (time, nodes)
    :param node_ids: 节点ID列表
    :param times: 时间点列表
    :param title: 图标题
    """
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))

    # 真实值热力图
    im1 = axes[0].imshow(true_values.T, aspect='auto', cmap='YlOrRd', origin='lower')
    axes[0].set_xlabel('Time Step')
    axes[0].set_ylabel('Node')
    axes[0].set_title('True Values')
    axes[0].set_yticks(range(len(node_ids)))
    axes[0].set_yticklabels(node_ids)
    plt.colorbar(im1, ax=axes[0])

    # 预测值热力图
    im2 = axes[1].imshow(pred_values.T, aspect='auto', cmap='YlOrRd', origin='lower')
    axes[1].set_xlabel('Time Step')
    axes[1].set_ylabel('Node')
    axes[1].set_title('Predicted Values')
    axes[1].set_yticks(range(len(node_ids)))
    axes[1].set_yticklabels(node_ids)
    plt.colorbar(im2, ax=axes[1])

    plt.suptitle(title)
    plt.tight_layout()
    plt.show()

def evaluate_st_model(y_true, y_pred):
    """
    评估时空预测模型
    :param y_true: 真实值 (samples, features, time, nodes)
    :param y_pred: 预测值 (samples, features, time, nodes)
    :return: 评估指标字典
    """
    y_true = y_true.reshape(-1)
    y_pred = y_pred.reshape(-1)

    rmse = np.sqrt(np.mean((y_true - y_pred) ** 2))
    mae = np.mean(np.abs(y_true - y_pred))
    mape = np.mean(np.abs((y_true - y_pred) / (y_true + 1e-8))) * 100

    return {
        'RMSE': rmse,
        'MAE': mae,
        'MAPE': mape
    }
```

---

## 预测算法选择指南

### 按数据规模选择

| 数据规模 | 推荐算法 | 说明 |
|---------|---------|------|
| 极小样本（<20） | 灰色预测、多项式拟合 | 小样本专用 |
| 小样本（20-100） | 灰色预测、SVR | 避免过拟合 |
| 中等样本（100-10000） | ARIMA、Prophet、回归 | 经典方法 |
| 大样本（>10000） | 神经网络、深度学习 | 充分学习 |

### 按数据特征选择

| 数据特征 | 推荐算法 |
|---------|---------|
| 单调增长/下降 | 灰色预测、趋势回归 |
| 季节性 + 趋势 | Holt-Winters、Prophet、SARIMA |
| 平稳时间序列 | ARIMA、指数平滑 |
| 非线性关系 | SVR、神经网络 |
| 多变量预测 | 多元回归、VAR、LSTM |
| 小样本 + 线性 | 线性回归 |
| 小样本 + 非线性 | SVR、RBF网络 |

### 按预测目标选择

| 预测目标 | 推荐算法 |
|---------|---------|
| 短期预测（<1周） | 指数平滑、ARIMA |
| 中期预测（1周-3月） | ARIMA、Prophet |
| 长期预测（>3月） | 灰色预测、神经网络 |
| 点预测 | 回归、SVR |
| 区间预测 | ARIMA、Prophet、贝叶斯方法 |
| 概率预测 | 贝叶斯方法、分位数回归 |

---

## 参考文献

1. 邓聚龙. (1990). *灰色系统理论教程*. 华中理工大学出版社.
2. Box, G. E., & Jenkins, G. M. (1970). *Time Series Analysis: Forecasting and Control*. Holden-Day.
3. Cortes, C., & Vapnik, V. (1995). Support-vector networks. *Machine Learning*, 20(3), 273-297.
4. Hochreiter, S., & Schmidhuber, J. (1997). Long short-term memory. *Neural Computation*, 9(8), 1735-1780.
5. Taylor, S. J., & Letham, B. (2018). Forecasting at scale. *The American Statistician*, 72(1), 37-45.
6. Hyndman, R. J., & Athanasopoulos, G. (2018). *Forecasting: principles and practice* (2nd ed.). OTexts.
7. Smola, A. J., & Schölkopf, B. (2004). A tutorial on support vector regression. *Statistics and Computing*, 14(3), 199-222.
8. Burden, R. L., & Faires, J. D. (2010). *Numerical Analysis* (9th ed.). Brooks/Cole.
