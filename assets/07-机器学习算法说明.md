# 机器学习算法说明文档

## 概述

机器学习算法是数学建模中的重要工具，用于从数据中学习模式并进行预测、分类和聚类。本章节涵盖集成学习、异常检测等常用机器学习算法。

---

## 1. 随机森林 (Random Forest)

### 算法介绍

随机森林是一种集成学习方法，通过构建多棵决策树并合并它们的预测结果来提高模型的泛化能力和稳定性。

#### 1.1 基本原理

**Bagging（Bootstrap Aggregating）**：
- 有放回抽样生成多个训练集
- 每个训练集训练一个基学习器
- 合并预测结果（投票或平均）

**随机特征选择**：
- 每个节点分裂时，随机选择一部分特征
- 增加树之间的多样性
- 减少方差

#### 1.2 回归随机森林

**预测**：
$$
\hat{y} = \frac{1}{T} \sum_{t=1}^{T} f_t(x)
$$

其中$T$是树的数量，$f_t(x)$是第$t$棵树的预测值。

**OOB误差（Out-of-Bag Error）**：
- 使用未被抽样的样本（袋外数据）验证
- 交叉验证的替代方案

#### 1.3 分类随机森林

**多数投票**：
$$
\hat{y} = \arg\max_{c} \sum_{t=1}^{T} \mathbb{I}(f_t(x) = c)
$$

**概率估计**：
$$
P(c|x) = \frac{1}{T} \sum_{t=1}^{T} \mathbb{I}(f_t(x) = c)
$$

#### 1.4 特征重要性

**Gini重要性**：
$$
GI_j = \frac{1}{T} \sum_{t=1}^{T} \Delta GI_{jt}
$$

其中$\Delta GI_{jt}$是特征$j$在树$t$中带来的Gini不纯度减少量。

**置换重要性**：
- 随机打乱某特征值
- 观察预测误差的变化
- 误差增加越多，特征越重要

### 适用范围

| 题型类型 | 典型问题 | 特征 |
|---------|---------|------|
| 分类预测 | 客户流失、疾病诊断 | 分类标签 |
| 回归预测 | 房价、销量预测 | 连续目标 |
| 特征选择 | 关键特征识别 | 高维数据 |
| 异常检测 | 欺诈检测 | 不平衡数据 |
| 数据缺失 | 含缺失值数据 | 鲁棒性强 |

### 可视化图表类型

- **特征重要性图**：各特征重要性排序
- **决策树可视化**：单棵树结构
- **OOB误差曲线**：误差随树数变化
- **偏依赖图**：特征对预测的影响
- **类别概率图**：分类概率分布

### 关键文献

| 论文名称 | 作者 | 年份 | 来源 |
|---------|------|------|------|
| Random Forests | Breiman | 2001 | Machine Learning |
| Classification and Regression Trees | Breiman et al. | 1984 | Wadsworth |

### 代码实现要点

```python
import numpy as np
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.metrics import accuracy_score, mean_squared_error
import matplotlib.pyplot as plt

def random_forest_classification(X_train, y_train, X_test, y_test,
                                  n_estimators=100, max_depth=None, random_state=42):
    """
    随机森林分类
    :param X_train: 训练特征
    :param y_train: 训练标签
    :param X_test: 测试特征
    :param y_test: 测试标签
    :return: 模型, 预测结果, OOB分数
    """
    rf = RandomForestClassifier(
        n_estimators=n_estimators,
        max_depth=max_depth,
        random_state=random_state,
        oob_score=True,
        n_jobs=-1
    )

    rf.fit(X_train, y_train)
    y_pred = rf.predict(X_test)
    y_proba = rf.predict_proba(X_test)

    accuracy = accuracy_score(y_test, y_pred)

    return rf, y_pred, y_proba, accuracy, rf.oob_score_

def random_forest_regression(X_train, y_train, X_test, y_test,
                              n_estimators=100, max_depth=None, random_state=42):
    """
    随机森林回归
    :param X_train: 训练特征
    :param y_train: 训练目标
    :param X_test: 测试特征
    :param y_test: 测试目标
    :return: 模型, 预测结果, MSE, OOB分数
    """
    rf = RandomForestRegressor(
        n_estimators=n_estimators,
        max_depth=max_depth,
        random_state=random_state,
        oob_score=True,
        n_jobs=-1
    )

    rf.fit(X_train, y_train)
    y_pred = rf.predict(X_test)

    mse = mean_squared_error(y_test, y_pred)

    return rf, y_pred, mse, rf.oob_score_

def plot_feature_importance(rf_model, feature_names, top_n=20):
    """
    绘制特征重要性
    :param rf_model: 随机森林模型
    :param feature_names: 特征名称列表
    :param top_n: 显示前N个特征
    """
    importances = rf_model.feature_importances_
    indices = np.argsort(importances)[::-1][:top_n]

    plt.figure(figsize=(10, 6))
    plt.bar(range(len(indices)), importances[indices])
    plt.xticks(range(len(indices)), [feature_names[i] for i in indices], rotation=90)
    plt.xlabel('Feature')
    plt.ylabel('Importance')
    plt.title('Feature Importance (Random Forest)')
    plt.tight_layout()
    plt.show()

def plot_oob_error(X, y, max_estimators=200):
    """
    绘制OOB误差随树数变化
    :param X: 特征矩阵
    :param y: 目标变量
    :param max_estimators: 最大树数
    """
    oob_errors = []

    for n in range(1, max_estimators + 1, 5):
        if len(np.unique(y)) <= 10:  # 分类
            rf = RandomForestClassifier(n_estimators=n, oob_score=True,
                                        random_state=42, n_jobs=-1)
            rf.fit(X, y)
            oob_errors.append(1 - rf.oob_score_)
        else:  # 回归
            rf = RandomForestRegressor(n_estimators=n, oob_score=True,
                                       random_state=42, n_jobs=-1)
            rf.fit(X, y)
            oob_errors.append(rf.oob_score_)

    plt.figure(figsize=(10, 6))
    plt.plot(range(1, max_estimators + 1, 5), oob_errors, 'o-')
    plt.xlabel('Number of Trees')
    plt.ylabel('OOB Error')
    plt.title('Random Forest: OOB Error vs Number of Trees')
    plt.grid(True, alpha=0.3)
    plt.show()

def plot_partial_dependence(rf_model, X, feature_names, feature_idx):
    """
    绘制偏依赖图
    :param rf_model: 随机森林模型
    :param X: 特征矩阵
    :param feature_names: 特征名称
    :param feature_idx: 特征索引
    """
    from sklearn.inspection import PartialDependenceDisplay

    PartialDependenceDisplay.from_estimator(rf_model, X, [feature_idx],
                                              feature_names=feature_names,
                                              kind='average')
    plt.show()
```

---

## 2. AdaBoost (Adaptive Boosting)

### 算法介绍

AdaBoost是一种自适应提升算法，通过迭代训练弱分类器并调整样本权重来构建强分类器。

#### 2.1 基本原理

**核心思想**：
- 训练一系列弱分类器
- 每轮关注之前分类错误的样本
- 组合多个弱分类器形成强分类器

**算法流程**：

1. 初始化样本权重：$w_i^{(1)} = 1/n$，$i = 1, 2, \ldots, n$

2. 对于$t = 1, 2, \ldots, T$：
   - 使用权重$w^{(t)}$训练弱分类器$h_t(x)$
   - 计算分类错误率：$\epsilon_t = \sum_{i=1}^{n} w_i^{(t)} \mathbb{I}(y_i \neq h_t(x_i))$
   - 计算分类器权重：$\alpha_t = \frac{1}{2} \ln\left(\frac{1-\epsilon_t}{\epsilon_t}\right)$
   - 更新样本权重：
     $$
     w_i^{(t+1)} = \frac{w_i^{(t)} \exp(-\alpha_t y_i h_t(x_i))}{\sum_{j=1}^{n} w_j^{(t)} \exp(-\alpha_t y_j h_t(x_j))}
     $$

3. 最终分类器：
   $$
   H(x) = \text{sign}\left( \sum_{t=1}^{T} \alpha_t h_t(x) \right)
   $$

#### 2.2 AdaBoost变体

**AdaBoost.M1**：二分类版本

**AdaBoost.M2**：多分类版本

**AdaBoost.R2**：回归版本

#### 2.3 梯度提升

**梯度提升机（GBM）**：
- 使用梯度下降方向训练新分类器
- 拟合负梯度（残差）
- 更灵活，可使用任意损失函数

**与AdaBoost关系**：
- AdaBoost是指数损失下的梯度提升
- GBM推广到任意可微损失函数

### 适用范围

| 题型类型 | 典型问题 | 特征 |
|---------|---------|------|
| 二分类 | 欺诈检测、垃圾邮件 | 标签不平衡 |
| 多分类 | 图像分类、文本分类 | 多类别 |
| 回归 | 价格预测、需求预测 | 连续目标 |
| 特征工程 | 特征选择 | 重要特征 |
| 集成学习 | 模型融合 | 提升性能 |

### 可视化图表类型

- **错误率曲线**：训练/测试错误率随轮数变化
- **分类器权重图**：各弱分类器权重
- **决策边界图**：分类边界可视化
- **特征重要性图**：特征重要性排序
- **残差图**：残差随轮数变化（回归）

### 关键文献

| 论文名称 | 作者 | 年份 | 来源 |
|---------|------|------|------|
| A decision-theoretic generalization of on-line learning | Freund & Schapire | 1995 | Journal of Computer and System Sciences |
| A short introduction to boosting | Schapire & Freund | 2012 | Journal of Japanese Society for Artificial Intelligence |
| Greedy Function Approximation: A Gradient Boosting Machine | Friedman | 2001 | Annals of Statistics |

### 代码实现要点

```python
import numpy as np
from sklearn.ensemble import AdaBoostClassifier, AdaBoostRegressor
from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor
from sklearn.metrics import accuracy_score, mean_squared_error
import matplotlib.pyplot as plt

def adaboost_classification(X_train, y_train, X_test, y_test,
                             n_estimators=50, learning_rate=1.0, random_state=42):
    """
    AdaBoost分类
    :param X_train: 训练特征
    :param y_train: 训练标签
    :param X_test: 测试特征
    :param y_test: 测试标签
    :param n_estimators: 弱分类器数量
    :param learning_rate: 学习率
    :return: 模型, 预测结果, 准确率, 错误率历史
    """
    # 基分类器：决策树桩（深度为1）
    base_clf = DecisionTreeClassifier(max_depth=1)

    ada = AdaBoostClassifier(
        estimator=base_clf,
        n_estimators=n_estimators,
        learning_rate=learning_rate,
        random_state=random_state
    )

    ada.fit(X_train, y_train)
    y_pred = ada.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)

    # 计算训练过程中的错误率
    train_errors = []
    for i, y_pred_train in enumerate(ada.staged_predict(X_train)):
        err = 1 - accuracy_score(y_train, y_pred_train)
        train_errors.append(err)

    test_errors = []
    for i, y_pred_test in enumerate(ada.staged_predict(X_test)):
        err = 1 - accuracy_score(y_test, y_pred_test)
        test_errors.append(err)

    return ada, y_pred, accuracy, train_errors, test_errors

def adaboost_regression(X_train, y_train, X_test, y_test,
                         n_estimators=50, learning_rate=1.0, loss='linear', random_state=42):
    """
    AdaBoost回归
    :param X_train: 训练特征
    :param y_train: 训练目标
    :param X_test: 测试特征
    :param y_test: 测试目标
    :param loss: 损失函数 ('linear', 'square', 'exponential')
    :return: 模型, 预测结果, MSE
    """
    base_reg = DecisionTreeRegressor(max_depth=3)

    ada = AdaBoostRegressor(
        estimator=base_reg,
        n_estimators=n_estimators,
        learning_rate=learning_rate,
        loss=loss,
        random_state=random_state
    )

    ada.fit(X_train, y_train)
    y_pred = ada.predict(X_test)
    mse = mean_squared_error(y_test, y_pred)

    # 计算训练过程中的MSE
    train_mse = []
    for y_pred_train in ada.staged_predict(X_train):
        mse_val = mean_squared_error(y_train, y_pred_train)
        train_mse.append(mse_val)

    test_mse = []
    for y_pred_test in ada.staged_predict(X_test):
        mse_val = mean_squared_error(y_test, y_pred_test)
        test_mse.append(mse_val)

    return ada, y_pred, mse, train_mse, test_mse

def plot_adaboost_errors(train_errors, test_errors, title="AdaBoost Error Rate"):
    """
    绘制AdaBoost错误率曲线
    :param train_errors: 训练错误率列表
    :param test_errors: 测试错误率列表
    :param title: 图标题
    """
    plt.figure(figsize=(10, 6))
    plt.plot(train_errors, 'o-', label='Train Error')
    plt.plot(test_errors, 's-', label='Test Error')
    plt.xlabel('Number of Estimators')
    plt.ylabel('Error Rate')
    plt.title(title)
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.show()

def plot_feature_importance_adaboost(ada_model, feature_names, top_n=20):
    """
    绘制特征重要性（AdaBoost）
    :param ada_model: AdaBoost模型
    :param feature_names: 特征名称
    :param top_n: 显示前N个特征
    """
    importances = ada_model.feature_importances_
    indices = np.argsort(importances)[::-1][:top_n]

    plt.figure(figsize=(10, 6))
    plt.bar(range(len(indices)), importances[indices])
    plt.xticks(range(len(indices)), [feature_names[i] for i in indices], rotation=90)
    plt.xlabel('Feature')
    plt.ylabel('Importance')
    plt.title('Feature Importance (AdaBoost)')
    plt.tight_layout()
    plt.show()

def plot_decision_boundary(model, X, y, title="Decision Boundary"):
    """
    绘制决策边界（适用于2D特征）
    :param model: 训练好的模型
    :param X: 特征矩阵
    :param y: 标签
    :param title: 图标题
    """
    if X.shape[1] != 2:
        print("Decision boundary plot only works for 2D features")
        return

    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),
                         np.arange(y_min, y_max, 0.1))

    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)

    plt.figure(figsize=(10, 6))
    plt.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.Paired)
    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired, edgecolors='k')
    plt.xlabel('Feature 1')
    plt.ylabel('Feature 2')
    plt.title(title)
    plt.tight_layout()
    plt.show()
```

---

## 3. 孤立森林 (Isolation Forest)

### 算法介绍

孤立森林是一种异常检测算法，通过随机分割特征空间来隔离数据点，异常点更容易被隔离（需要更少的分割步骤）。

#### 3.1 基本原理

**核心思想**：
- 异常点是少数且不同的
- 异常点更容易被孤立
- 使用路径长度衡量异常程度

**孤立树（iTree）构建**：

1. 随机选择特征和分割值
2. 递归分割数据空间
3. 直到：所有数据点被孤立 或 达到最大深度

**路径长度**：
$$
h(x) = \sum_{i=1}^{\text{path}} \Delta i + C(\psi)
$$

其中$\Delta i$是节点到子节点的路径长度，$C(\psi)$是调整项。

#### 3.2 异常得分

**归一化路径长度**：
$$
H(x) = h(x) / c(\psi)
$$

其中$c(\psi) = 2H(\psi - 1) - (2(\psi - 1)/\psi)$是调整路径长度，$\psi$是样本数。

**异常得分**：
$$
s(x, \psi) = 2^{-\frac{E[H(x)]}{c(\psi)}}
$$

**得分解释**：
- $s \approx 1$：异常点
- $s < 0.5$：正常点
- $s \approx 0.5$：无异常

#### 3.3 算法特点

**优点**：
- 无需假设数据分布
- 线性时间复杂度
- 内存效率高
- 可处理高维数据

**参数**：
- `n_estimators`：树的数量
- `max_samples`：每棵树的样本数
- `contamination`：异常比例

### 适用范围

| 题型类型 | 典型问题 | 特征 |
|---------|---------|------|
| 欺诈检测 | 信用卡欺诈 | 极度不平衡 |
| 网络入侵 | 异常流量 | 异常模式 |
| 设备故障 | 设备异常检测 | 传感器数据 |
| 数据清洗 | 异常值识别 | 数据质量 |
| 医疗诊断 | 罕见病识别 | 罕见模式 |

### 可视化图表类型

- **异常得分图**：各样本异常得分
- **异常散点图**：异常点在特征空间的分布
- **路径长度分布**：路径长度直方图
- **ROC曲线**：检测性能评估
- **2D投影图**：二维特征空间的异常点

### 关键文献

| 论文名称 | 作者 | 年份 | 来源 |
|---------|------|------|------|
| Isolation Forest | Liu et al. | 2008 | ICDM |
| Isolation-based Anomaly Detection | Liu et al. | 2011 | ACM TKDD |

### 代码实现要点

```python
import numpy as np
from sklearn.ensemble import IsolationForest
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, roc_auc_score, roc_curve
import matplotlib.pyplot as plt

def isolation_forest_anomaly_detection(X, contamination=0.1, random_state=42):
    """
    孤立森林异常检测
    :param X: 特征矩阵
    :param contamination: 异常比例
    :return: 模型, 异常标签, 异常得分
    """
    iso_forest = IsolationForest(
        n_estimators=100,
        max_samples='auto',
        contamination=contamination,
        random_state=random_state,
        n_jobs=-1
    )

    iso_forest.fit(X)

    # 预测：-1表示异常，1表示正常
    labels = iso_forest.predict(X)

    # 计算异常得分
    scores = iso_forest.score_samples(X)
    # 转换为0-1范围（越大越异常）
    anomaly_scores = -scores

    return iso_forest, labels, anomaly_scores

def plot_anomaly_scores(X, anomaly_scores, labels, feature_names=None):
    """
    绘制异常得分可视化
    :param X: 特征矩阵
    :param anomaly_scores: 异常得分
    :param labels: 异常标签
    :param feature_names: 特征名称
    """
    if X.shape[1] < 2:
        print("Need at least 2 features for visualization")
        return

    fig, axes = plt.subplots(1, 2, figsize=(14, 6))

    # 散点图（使用前两个特征）
    normal_mask = labels == 1
    anomaly_mask = labels == -1

    axes[0].scatter(X[normal_mask, 0], X[normal_mask, 1],
                   c='blue', label='Normal', alpha=0.5)
    axes[0].scatter(X[anomaly_mask, 0], X[anomaly_mask, 1],
                   c='red', label='Anomaly', alpha=0.8)
    axes[0].set_xlabel(feature_names[0] if feature_names else 'Feature 1')
    axes[0].set_ylabel(feature_names[1] if feature_names else 'Feature 2')
    axes[0].set_title('Anomaly Detection Results')
    axes[0].legend()
    axes[0].grid(True, alpha=0.3)

    # 异常得分分布
    axes[1].hist(anomaly_scores[normal_mask], bins=50, alpha=0.5,
                label='Normal', color='blue')
    axes[1].hist(anomaly_scores[anomaly_mask], bins=50, alpha=0.5,
                label='Anomaly', color='red')
    axes[1].set_xlabel('Anomaly Score')
    axes[1].set_ylabel('Frequency')
    axes[1].set_title('Anomaly Score Distribution')
    axes[1].legend()
    axes[1].grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()

def evaluate_anomaly_detection(y_true, y_pred, scores):
    """
    评估异常检测性能
    :param y_true: 真实标签（1正常，-1异常）
    :param y_pred: 预测标签
    :param scores: 异常得分
    :return: 分类报告, AUC
    """
    # 转换标签为0/1格式
    y_true_binary = (y_true == -1).astype(int)
    y_pred_binary = (y_pred == -1).astype(int)

    print("Classification Report:")
    print(classification_report(y_true_binary, y_pred_binary,
                                target_names=['Normal', 'Anomaly']))

    auc = roc_auc_score(y_true_binary, scores)

    return auc

def plot_roc_curve(y_true, scores):
    """
    绘制ROC曲线
    :param y_true: 真实标签（1正常，-1异常）
    :param scores: 异常得分
    """
    from sklearn.metrics import roc_curve, auc

    y_true_binary = (y_true == -1).astype(int)

    fpr, tpr, thresholds = roc_curve(y_true_binary, scores)
    roc_auc = auc(fpr, tpr)

    plt.figure(figsize=(8, 6))
    plt.plot(fpr, tpr, 'b-', label=f'ROC curve (AUC = {roc_auc:.2f})')
    plt.plot([0, 1], [0, 1], 'r--', label='Random')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC Curve - Anomaly Detection')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.show()

    return roc_auc

def plot_path_length_distribution(iso_forest, X, max_samples=1000):
    """
    绘制路径长度分布
    :param iso_forest: 训练好的孤立森林模型
    :param X: 特征矩阵
    :param max_samples: 采样数量
    """
    # 计算路径长度
    path_lengths = iso_forest.score_samples(X[:max_samples]) * -1

    plt.figure(figsize=(10, 6))
    plt.hist(path_lengths, bins=50, alpha=0.7, edgecolor='black')
    plt.xlabel('Path Length')
    plt.ylabel('Frequency')
    plt.title('Distribution of Path Lengths')
    plt.grid(True, alpha=0.3)
    plt.show()

def contamination_sensitivity_analysis(X, y, contamination_range):
    """
    污染率敏感性分析
    :param X: 特征矩阵
    :param y: 真实标签
    :param contamination_range: 污染率范围
    :return: 各污染率下的性能指标
    """
    from sklearn.metrics import f1_score, precision_score, recall_score

    f1_scores = []
    precisions = []
    recalls = []

    for cont in contamination_range:
        _, labels, _ = isolation_forest_anomaly_detection(X, contamination=cont)

        y_binary = (y == -1).astype(int)
        y_pred_binary = (labels == -1).astype(int)

        f1 = f1_score(y_binary, y_pred_binary)
        prec = precision_score(y_binary, y_pred_binary, zero_division=0)
        rec = recall_score(y_binary, y_pred_binary, zero_division=0)

        f1_scores.append(f1)
        precisions.append(prec)
        recalls.append(rec)

    plt.figure(figsize=(10, 6))
    plt.plot(contamination_range, f1_scores, 'o-', label='F1 Score')
    plt.plot(contamination_range, precisions, 's-', label='Precision')
    plt.plot(contamination_range, recalls, '^-', label='Recall')
    plt.xlabel('Contamination Rate')
    plt.ylabel('Score')
    plt.title('Sensitivity Analysis: Contamination Rate')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.show()

    return f1_scores, precisions, recalls
```

---

## 机器学习算法选择指南

### 按问题类型选择

| 问题类型 | 推荐算法 | 说明 |
|---------|---------|------|
| 分类问题 | 随机森林、AdaBoost | 集成学习方法 |
| 回归问题 | 随机森林 | 鲁棒性强 |
| 异常检测 | 孤立森林 | 无监督检测 |
| 特征选择 | 随机森林、AdaBoost | 特征重要性 |
| 高维数据 | 随机森林 | 特征随机选择 |
| 不平衡数据 | AdaBoost、随机森林 | 可调整权重 |

### 按数据特征选择

| 数据特征 | 推荐算法 |
|---------|---------|
| 含缺失值 | 随机森林、XGBoost |
| 高维度 | 随机森林、孤立森林 |
| 小样本 | 随机森林（OOB验证） |
| 非线性关系 | 随机森林、AdaBoost |
| 需要概率输出 | 随机森林、AdaBoost |
| 噪声较多 | 随机森林 |

### 按解释性要求选择

| 解释性要求 | 推荐算法 |
|-----------|---------|
| 需要特征重要性 | 随机森林、AdaBoost |
| 需要决策规则 | 随机森林（单棵树） |
| 需要预测概率 | 随机森林、AdaBoost |
| 黑箱可接受 | 所有集成学习 |

---

## 参考文献

1. Breiman, L. (2001). Random Forests. *Machine Learning*, 45(1), 5-32.
2. Freund, Y., & Schapire, R. E. (1997). A decision-theoretic generalization of on-line learning and an application to boosting. *Journal of Computer and System Sciences*, 55(1), 119-139.
3. Liu, F. T., Ting, K. M., & Zhou, Z. H. (2008). Isolation Forest. *Proceedings of the 2008 Eighth IEEE International Conference on Data Mining*, 413-422.
4. Breiman, L., Friedman, J., Stone, C. J., & Olshen, R. A. (1984). *Classification and Regression Trees*. Wadsworth.
5. Friedman, J. H. (2001). Greedy Function Approximation: A Gradient Boosting Machine. *Annals of Statistics*, 29(5), 1189-1232.
